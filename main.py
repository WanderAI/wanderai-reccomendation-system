from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
from pydantic import BaseModel, constr, validator
from enum import Enum

app = FastAPI()

# -*- coding: utf-8 -*-
"""Recommendation System WORKING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13EWc0Pwm0XScyOSSf9epGtVUruawTrGu

## Dataset and requirements import

### NOTE: The datasets used here are already cleaned. The cleaning process can be seen at "Dataset Restaurants and Acommodations Cleaning and Combining.ipynb"
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd '/usr/local/lib/python3.10/dist-packages'
# !python3 -m pip install PySastrawi

# !pip install nltk

# !pip install haversine

import pandas as pd
import os 
import Sastrawi.StopWordRemover.StopWordRemoverFactory as swrf
import nltk 
from haversine import haversine, Unit

"""ALL of these datasets have been cleaned, and ready to be used"""

dataset_tourism = pd.read_csv('_USE_THIS_tourism_final_clean_photo_cost.csv')
dataset_restaurant = pd.read_csv('_USE_THIS_restaurant_final_clean_cost_range.csv')
dataset_acommodation = pd.read_csv('_USE_THIS_akomodasi_final_clean.csv')


"""## Cosine similarity search function

TF-IDF Vectorization
"""

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import numpy as np
tfidf=TfidfVectorizer()

def query_preprocess(query):
  print("Cleaning query. Lowercaseing, removing stopwords, ")
  query = query.lower()
  query = re.sub("[^-9A-Za-z ]", "" , query)

  factory = swrf.StopWordRemoverFactory()
  stopword = factory.create_stop_word_remover()
  processed_query = stopword.remove(query)
  return processed_query

# The main heavy-lifting is done by this func
# RETURNS a list of cosine similarity scores between the new query and already existing
def recommendation_with_query(data, tfidf_vectorizer, deskripsi_tempat, kota):
  print("..STARTING RECOMMENDATION PROCESS..")
  print("Processing data and query")
  # gabung jadi satu query
  processed_query = query_preprocess(deskripsi_tempat + " " + kota)
  query = [processed_query]

  # ambil dataset bagian index dan features saja
  dataset = data[['index','features']]

  # bikin query ke bentuk dataframe
  data_len = int(data.shape[0])
  new_entry = pd.DataFrame({'index': data_len, 'features': query})
  # print(data_len)
  # print(new_entry)

  # tambahkan ke original dataset
  dataset_concat = pd.concat([dataset, new_entry], axis=0, ignore_index=True)
  # print("dataset_concat",dataset_concat.tail())

  # lakukan fitting TF-IDF 
  print("Vectorization data")
  vectorized_concated = tfidf_vectorizer.fit_transform(dataset_concat["features"].apply(lambda x: np.str_(x)))
  
  # cari cosine similarity nya
  print("Cosine similarity data")
  cosine_similar_list = cosine_similarity(vectorized_concated) 

  print("...generated scores of cosine similarity")
  # returns the list of cosine simlarity scores between each element, and ID of the query 
  return cosine_similar_list, data_len

def get_the_first_n_recommendation_places_and_scores(data, cosine_sim_list, place_index, n_count):
    print("FILTERING BASED ON COSINE SIMILARITY SCORES")

    # Get the recommendation based on the highest score (most similar)
    scores=list(enumerate(cosine_sim_list[place_index]))
    sorted_scores=sorted(scores,key=lambda x:x[1],reverse=True)

    # not using the score against itself, which is always 1 (perfect similarity)
    # take only the n_count highest scores
    sorted_scores=sorted_scores[1:n_count]
    # print(sorted_scores)
    index_list_from_data = []
    # returns all of the attributes of the selected places
    for scores in sorted_scores:
      index_list_from_data.append(scores[0])
    
    print("taking the first several places based on number of days")
    places_recommendations = data.iloc[index_list_from_data]

    print("..generated places recommendations and their respective cosine scores")
    return  places_recommendations, sorted_scores

"""### Filter by Location & Cost

"""

def filter_result_by_location_cost(places_list, city, cost):
  # Return filtered places_list
  print("FILTERING RECOMMENDED PLACES BASED ON CITY NAME AND COST CATEGORY")
  filtered_places = places_list.copy()
  filtered_places = filtered_places[filtered_places['City'] == city]
  filtered_places = filtered_places[filtered_places['LevelPrice'] <= cost]
  print("FILTERING #1 DONE")
  return filtered_places


"""### Clustering the closest groups of places"""

from sklearn.cluster import KMeans

def clustering_places_kmeans(places_recc_list, n_days):
  dataset_cluster = places_recc_list[['index','Lat','Long']]
  print("CLUSTERING PLACES BASED ON THEIR LOCATION")

  kmeans = KMeans(n_clusters = n_days, init ='k-means++')
  kmeans.fit(dataset_cluster[['Lat','Long']]) # Compute k-means clustering.
  dataset_cluster['cluster_label'] = kmeans.fit_predict(dataset_cluster[['Lat','Long']])
  centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
  labels = kmeans.predict(dataset_cluster[['Lat','Long']]) # Labels of each point

  places_recc_list['cluster_label'] = labels
  # places_recc_list['cluster_label'] = labels
  dataset_clustered = places_recc_list.copy()
  print("CLUSTERING DONE")
  return dataset_clustered, centers, labels

# dataset_clustered, centroid, labels = clustering_places_kmeans(places_list)


"""### Filter jumlah tempat per hari"""

# Find the lowest amount of places inside a cluster
# Make that the num of places to visit each day
def filter_num_of_places_in_a_day_sorted(dataset_cluster):
  count_min={}
  print("FILTERING BASED ON NUM OF PLACES IN A DAY")
  # get the minimum place in a day
  print("get the minimum place in a day")
  for element in dataset_cluster['cluster_label'].unique():
    count_min[element] = dataset_cluster[dataset_cluster['cluster_label']==element]['index'].count()
    print(f"{element}: {dataset_cluster[dataset_cluster['cluster_label']==element]['index'].count()} ")
  min_places_in_a_cluster = count_min[min(count_min, key=count_min.get)]

  # split df based on the days
  df_list_based_on_day = []
  for uniq in dataset_cluster['cluster_label'].unique():
    df_list_based_on_day.append( dataset_cluster[dataset_cluster['cluster_label']==uniq] )

  # remove element randomly 
  print("deleting excessive places for each day")
  df_list_based_on_day_cleaned = []
  for df in df_list_based_on_day:
    # iterate over each day's df
    df = df.sample(frac=1).reset_index(drop=True)  # shuffle dataset
    df = df[:min_places_in_a_cluster]  # take the first minimum number of place
    df_list_based_on_day_cleaned.append(df)
  
  # return the filtered dataframe
  num_of_places_per_day = min_places_in_a_cluster
  print("FILTERING #2 DONE")
  return df_list_based_on_day_cleaned, num_of_places_per_day

# tourism_lists_each_day, n_of_places_per_day =  filter_num_of_places_in_a_day_sorted(dataset_clustered)

"""### Combine each days with the surrounding restaurant, with respect to their cost and ratings and reviews

Filter by cost and ratings
"""

def filter_restaurants_by_cost_ratings(restaurant_dataset, city, cost):
    print("filtering based on city and cost")
    restaurant_dataset = restaurant_dataset[restaurant_dataset['lokasi_kota']==city]
    restaurant_dataset = restaurant_dataset[restaurant_dataset['level_price']<=cost]
    return restaurant_dataset

"""Get closests for each centroid"""


def get_distance(lati1, long1, lati2, long2):
    return haversine((lati1, long1), (lati2, long2), unit=Unit.KILOMETERS)

def get_restaurants_each_day(place_recommendation_dataset, centroid, restaurant_dataset_original, city, cost):
    print("GETTING RESTAURANTS RECOMMENDATION")
    restaurant_dataset = filter_restaurants_by_cost_ratings(restaurant_dataset_original, city, cost)

    print("finding nearests restaurants for each day")
    resturants_for_each_day = []
    cluster_label_order = [] 
    # get the list of centroid order
    print('centroids ordering')
    for day in place_recommendation_dataset:
      # get the first one on every day
      cluster_label_order.append(day.iloc[0]['cluster_label'])
      print(day.iloc[0]['cluster_label'])
      
    for label in cluster_label_order:
      # for index, place in day.iterrows(): 
      lat_tourism = centroid[label][0]
      long_tourism = centroid[label][1]

      # finding the distance for every centroid and restaurant_place
      # assign it to a new row in the restaurant dataset
      restaurant_dataset[f"distance_cluster_{label}"] = restaurant_dataset.apply(
          lambda x: get_distance(x['geometry_location_lat'],x['geometry_location_lng'],
                                  lat_tourism, long_tourism) ,axis=1)
    # create a feature which averages the distance to all centroids
    restaurant_dataset["distance_avg"] = restaurant_dataset.apply(
          lambda x: np.mean( [ x[f"distance_cluster_{i}"]  for i in cluster_label_order ]) ,axis=1)
    
    for label in cluster_label_order:
      # sort by the closest to the centroid
      restaurant_dataset.sort_values(f"distance_cluster_{label}",axis=0,inplace=True)
      # make a new columns to save the distance to this cluster/day
      restaurant_dataset["distance_part_of_cluster"] = restaurant_dataset.apply(lambda x: x[f"distance_cluster_{label}"], axis=1) 

      # take 4 closest for each day  
      resturants_for_each_day.append(restaurant_dataset.iloc[:4]) 
      # to ensure no duplicate choice
      restaurant_dataset.drop(restaurant_dataset.index[0:4], inplace=True)
    
    print("...generated restaurants recommendations for each day")
    return resturants_for_each_day


"""### Find the closest surrounding hotels/accomodations to all of the places, not just each day

Filter by cost and ratings
"""

def filter_acommodations_by_cost_ratings(acommodations_dataset, city, cost):
  print("filtering based on city and cost")
  acommodations_dataset = acommodations_dataset[acommodations_dataset['lokasi_kota']==city]
  acommodations_dataset = acommodations_dataset[acommodations_dataset['level_price']<=cost]
  return acommodations_dataset

def get_acommodations(place_recommendation_dataset, centroid, acommodation_dataset_original, city, cost):
    print("GETTING ACOMMODATIONS RECOMMENDATION")
    acommodation_dataset = filter_restaurants_by_cost_ratings(acommodation_dataset_original, city, cost)

    print("finding nearests acommodations")
    cluster_label_order = [] 
    # get the list of centroid order
    print('centroids ordering')
    for day in place_recommendation_dataset:
      # get the first one on every day
      cluster_label_order.append(day.iloc[0]['cluster_label'])
      print(day.iloc[0]['cluster_label'])
      
    for label in cluster_label_order:
      # for index, place in day.iterrows(): 
      lat_tourism = centroid[label][0]
      long_tourism = centroid[label][1]

      # finding the distance for every centroid and restaurant_place
      # assign it to a new row in the restaurant dataset
      acommodation_dataset[f"distance_cluster_{label}"] = acommodation_dataset.apply(
          lambda x: get_distance(x['geometry_location_lat'],x['geometry_location_lng'],
                                  lat_tourism, long_tourism) ,axis=1)
    # create a feature which averages the distance to all centroids
    acommodation_dataset["distance_avg"] = acommodation_dataset.apply(
          lambda x: np.mean( [ x[f"distance_cluster_{i}"]  for i in cluster_label_order ]) ,axis=1)
    
  
    # sort by the closest average distance to all centroid
    acommodation_dataset.sort_values(f"distance_avg",axis=0,inplace=True)

    # take the first 7
    print("...generated acommodations recommendations")
    return acommodation_dataset[:10]

# acommodations_recommendations =  get_acommodations(tourism_lists_each_day, centroid, dataset_acommodation, CITY, COST)

"""### Calculate cost minimum and maximum"""

def calculate_cost_min_max(tourism_recommendations_each_day, restaurants_recommendations_each_day):
  # calculate cost for tourism
  print("CALCULATING COSTS")
  min_tourism = 0
  max_tourism = 0
  # print(cost_sheets)
  for df in tourism_recommendations_each_day:
    for index,place in df.iterrows():
        min_tourism += place['cost_range_min']
        max_tourism += place['cost_range_max']

  print("min_tourism",min_tourism)
  print("max_tourism",max_tourism)
  min_restaurant = 0
  max_restaurant = 0

  # calculate cost for restaurants
  for df in restaurants_recommendations_each_day:
    for index,place in df.iterrows():
        min_restaurant += place['cost_range_min']
        max_restaurant += place['cost_range_max']
  print("min_restaurant",min_restaurant)
  print("max_restaurant",max_restaurant)
  total_min_cost =  min_tourism + min_restaurant
  total_max_cost =  max_tourism + max_restaurant

  print()
  print("total_min_cost",total_min_cost)
  print("total_max_cost",total_max_cost)
  return total_min_cost, total_max_cost

# calculate_cost_min_max(tourism_lists_each_day, restaurants_recommendations_each_day)

"""### FINAL PREDICTION COMBINATION FLOW"""

# Take input query, location, n_days, cost_level
def predict_give_places_recommendations(dataset_tourism, dataset_restaurant, dataset_acommodation, query, city, n_days, n_people, cost):
  tfidf = TfidfVectorizer()

  # filtering #1, city and cost
  dataset_tourism = filter_result_by_location_cost(dataset_tourism, city, cost)

  # clean query and give recommends with content-based filtering
  cos_list, id = recommendation_with_query(dataset_tourism, tfidf, query, city)

  # take the first n recommendations
  places_list, sorted_scores_list =  get_the_first_n_recommendation_places_and_scores(dataset_tourism, cos_list, id, n_days*8)


  # clustering
  dataset_clustered, centroid, labels = clustering_places_kmeans(places_list, n_days)

  # filtering #2, n of day
  tourism_lists_each_day, num_of_places_per_day = filter_num_of_places_in_a_day_sorted(dataset_clustered)
  
  # give restaurants recommendations for each day, filtered by cost and distance 
  restaurants_recommendations_each_day = get_restaurants_each_day(tourism_lists_each_day, centroid, dataset_restaurant, city, cost)
  # give acommodations recommendations, filtered by cost
  acommodations_recommendations =  get_acommodations(tourism_lists_each_day, centroid, dataset_acommodation, city, cost)

  # calculate cost
  cost_minimum, cost_maximum = calculate_cost_min_max(tourism_lists_each_day, restaurants_recommendations_each_day)

  # select the output columns
  tourism_lists_each_day_fin = []
  restaurants_recommendations_each_day_fin = []
  for df in tourism_lists_each_day:
    tourism_lists_each_day_fin.append(df[['place_id', 'Place_Name', 'image_link', 'Description', 'Category', 'City', 'Rating', 'Lat', 'Long', 'cost_range_min', 'cost_range_max']].to_dict(orient='records'))

  for df in restaurants_recommendations_each_day:
    restaurants_recommendations_each_day_fin.append(df[['place_id', 'link_restaurant', 'name', 'formatted_address', 'geometry_location_lat', 'geometry_location_lng', 'tipe_makanan', 'level_price', 'rating_ten', 'cost_range_min', 'cost_range_max', 'distance_part_of_cluster']].to_dict(orient='records'))

  acommodations_recommendations = acommodations_recommendations[['place_icon_image', 'acommodation_name_concatenated', 'formatted_address', 'geometry_location_lat', 'geometry_location_lng', 'acommodation_type', 'lokasi', 'rate_level', 'rating.1', 'num_of_reviews', 'price_per_night', 'distance_avg']].to_dict(orient='records')

  print("...RECOMMENDATION PROCESS DONE")

  # Create a dictionary to represent the JSON object
  result = {
    'tourism_lists_each_day': tourism_lists_each_day_fin,
    'restaurants_recommendations_each_day': restaurants_recommendations_each_day_fin,
    'accommodations_recommendations': acommodations_recommendations,
    'cost_minimum': cost_minimum,
    'cost_maximum': cost_maximum,
    'total_cost_minimum': cost_minimum * n_people,
    'total_cost_maximum': cost_maximum * n_people
  }

  # Return the JSON object
  return result


class ReccomendRequest(BaseModel):
    query: str
    city: str
    n_days: int
    n_people: int
    cost: int

    class Config:
        schema_extra = {
            "example": {
                "query": "Tempat untuk menikmati suasana alam",
                "city": "Bandung",
                "n_days": 5,
                "n_people": 2,
                "cost": 3
            }
        }
    

@app.get("/")
async def home():
    return {"message": "Welcome to Reccomendation API!"}

@app.post("/recommend")
async def recommend_places(inputUser: ReccomendRequest):
    # Format the city input
    formatted_city = inputUser.city.lower().capitalize()

    try:
        # Validate the input
        if len(inputUser.query) > 256:
            return JSONResponse(
                status_code=400,
                content={
                            "message": "Query exceeding limit",
                            "detail": "Query must not exceed 256 characters."
                        }
            )

        if formatted_city not in ["Jakarta", "Bandung", "Yogyakarta", "Semarang", "Surabaya"]:
            return JSONResponse(
                status_code=400,
                content={
                            "message": "Invalid city",
                            "detail": "Invalid city. Must be one of Jakarta, Bandung, Yogyakarta, Semarang, or Surabaya."
                        }
            )

        if inputUser.n_days < 1 or inputUser.n_days > 13:
            return JSONResponse(
                status_code=400,
                content={
                            "message": "Invalid n_days",
                            "detail": "Invalid n_days. Must be between 1 and 13."
                        }
            )

        if inputUser.cost not in [1, 2, 3, 4]:
            return JSONResponse(
                status_code=400,
                content={
                            "message": "Invalid cost",
                            "detail": "Invalid cost. Must be one of 1, 2, 3, or 4."
                        }
            )

        if inputUser.n_people < 1:
            return JSONResponse(
                status_code=400,
                content={
                            "message": "Invalid n_people",
                            "detail": "Invalid n_people. Must be at least 1."
                        }
            )

        # Run recommendation system
        result = predict_give_places_recommendations(
            dataset_tourism,
            dataset_restaurant,
            dataset_acommodation,
            inputUser.query,
            formatted_city,
            inputUser.n_days,
            inputUser.n_people,
            inputUser.cost
        )

        if result is None:
            return JSONResponse(
                status_code=209,
                content={"message": "Data not found"}
            )

        return result

    except Exception as e:
        return JSONResponse(
                status_code=209,
                content={"message": "Data not found"}
            )


if __name__ == '__main__':
    uvicorn.run('main:app', host='0.0.0.0', port=8000,
                log_level="info")