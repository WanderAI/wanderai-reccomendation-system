# -*- coding: utf-8 -*-
"""Recommendation System WORKING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13EWc0Pwm0XScyOSSf9epGtVUruawTrGu

## Dataset and requirements import

### NOTE: The datasets used here are already cleaned. The cleaning process can be seen at "Dataset Restaurants and Acommodations Cleaning and Combining.ipynb"
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd '/usr/local/lib/python3.10/dist-packages'
# !python3 -m pip install PySastrawi

# !pip install nltk

# !pip install haversine

import pandas as pd
import os 
import Sastrawi.StopWordRemover.StopWordRemoverFactory as swrf
import nltk 
from haversine import haversine, Unit

"""ALL of these datasets have been cleaned, and ready to be used"""

dataset_tourism = pd.read_csv('_USE_THIS_tourism_final_clean_photo_cost.csv')
dataset_restaurant = pd.read_csv('_USE_THIS_restaurant_final_clean_cost_range.csv')
dataset_acommodation = pd.read_csv('_USE_THIS_akomodasi_final_clean.csv')


"""## Cosine similarity search function

TF-IDF Vectorization
"""

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import numpy as np
tfidf=TfidfVectorizer()

def query_preprocess(query):
  print("Cleaning query. Lowercaseing, removing stopwords, ")
  query = query.lower()
  query = re.sub("[^-9A-Za-z ]", "" , query)

  factory = swrf.StopWordRemoverFactory()
  stopword = factory.create_stop_word_remover()
  processed_query = stopword.remove(query)
  return processed_query

# The main heavy-lifting is done by this func
# RETURNS a list of cosine similarity scores between the new query and already existing
def recommendation_with_query(data, tfidf_vectorizer, deskripsi_tempat, kota):
  print("..STARTING RECOMMENDATION PROCESS..")
  print("Processing data and query")
  # gabung jadi satu query
  processed_query = query_preprocess(deskripsi_tempat + " " + kota)
  query = [processed_query]

  # ambil dataset bagian index dan features saja
  dataset = data[['index','features']]

  # bikin query ke bentuk dataframe
  data_len = int(data.shape[0])
  new_entry = pd.DataFrame({'index': data_len, 'features': query})
  # print(data_len)
  # print(new_entry)

  # tambahkan ke original dataset
  dataset_concat = pd.concat([dataset, new_entry], axis=0, ignore_index=True)
  # print("dataset_concat",dataset_concat.tail())

  # lakukan fitting TF-IDF 
  print("Vectorization data")
  vectorized_concated = tfidf_vectorizer.fit_transform(dataset_concat["features"].apply(lambda x: np.str_(x)))
  
  # cari cosine similarity nya
  print("Cosine similarity data")
  cosine_similar_list = cosine_similarity(vectorized_concated) 

  print("...generated scores of cosine similarity")
  # returns the list of cosine simlarity scores between each element, and ID of the query 
  return cosine_similar_list, data_len

def get_the_first_n_recommendation_places_and_scores(data, cosine_sim_list, place_index, n_count):
    print("FILTERING BASED ON COSINE SIMILARITY SCORES")

    # Get the recommendation based on the highest score (most similar)
    scores=list(enumerate(cosine_sim_list[place_index]))
    sorted_scores=sorted(scores,key=lambda x:x[1],reverse=True)

    # not using the score against itself, which is always 1 (perfect similarity)
    # take only the n_count highest scores
    sorted_scores=sorted_scores[1:n_count]
    # print(sorted_scores)
    index_list_from_data = []
    # returns all of the attributes of the selected places
    for scores in sorted_scores:
      index_list_from_data.append(scores[0])
    
    print("taking the first several places based on number of days")
    places_recommendations = data.iloc[index_list_from_data]

    print("..generated places recommendations and their respective cosine scores")
    return  places_recommendations, sorted_scores

"""### Filter by Location & Cost

"""

def filter_result_by_location_cost(places_list, city, cost):
  # Return filtered places_list
  print("FILTERING RECOMMENDED PLACES BASED ON CITY NAME AND COST CATEGORY")
  filtered_places = places_list.copy()
  filtered_places = filtered_places[filtered_places['City'] == city]
  filtered_places = filtered_places[filtered_places['LevelPrice'] <= cost]
  print("FILTERING #1 DONE")
  return filtered_places


"""### Clustering the closest groups of places"""

from sklearn.cluster import KMeans

def clustering_places_kmeans(places_recc_list, n_days):
  dataset_cluster = places_recc_list[['index','Lat','Long']]
  print("CLUSTERING PLACES BASED ON THEIR LOCATION")

  kmeans = KMeans(n_clusters = n_days, init ='k-means++')
  kmeans.fit(dataset_cluster[['Lat','Long']]) # Compute k-means clustering.
  dataset_cluster['cluster_label'] = kmeans.fit_predict(dataset_cluster[['Lat','Long']])
  centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
  labels = kmeans.predict(dataset_cluster[['Lat','Long']]) # Labels of each point

  places_recc_list['cluster_label'] = labels
  # places_recc_list['cluster_label'] = labels
  dataset_clustered = places_recc_list.copy()
  print("CLUSTERING DONE")
  return dataset_clustered, centers, labels

# dataset_clustered, centroid, labels = clustering_places_kmeans(places_list)


"""### Filter jumlah tempat per hari"""

# Find the lowest amount of places inside a cluster
# Make that the num of places to visit each day
def filter_num_of_places_in_a_day_sorted(dataset_cluster):
  count_min={}
  print("FILTERING BASED ON NUM OF PLACES IN A DAY")
  # get the minimum place in a day
  print("get the minimum place in a day")
  for element in dataset_cluster['cluster_label'].unique():
    count_min[element] = dataset_cluster[dataset_cluster['cluster_label']==element]['index'].count()
    print(f"{element}: {dataset_cluster[dataset_cluster['cluster_label']==element]['index'].count()} ")
  min_places_in_a_cluster = count_min[min(count_min, key=count_min.get)]

  # split df based on the days
  df_list_based_on_day = []
  for uniq in dataset_cluster['cluster_label'].unique():
    df_list_based_on_day.append( dataset_cluster[dataset_cluster['cluster_label']==uniq] )

  # remove element randomly 
  print("deleting excessive places for each day")
  df_list_based_on_day_cleaned = []
  for df in df_list_based_on_day:
    # iterate over each day's df
    df = df.sample(frac=1).reset_index(drop=True)  # shuffle dataset
    df = df[:min_places_in_a_cluster]  # take the first minimum number of place
    df_list_based_on_day_cleaned.append(df)
  
  # return the filtered dataframe
  num_of_places_per_day = min_places_in_a_cluster
  print("FILTERING #2 DONE")
  return df_list_based_on_day_cleaned, num_of_places_per_day

# tourism_lists_each_day, n_of_places_per_day =  filter_num_of_places_in_a_day_sorted(dataset_clustered)

"""### Combine each days with the surrounding restaurant, with respect to their cost and ratings and reviews

Filter by cost and ratings
"""

def filter_restaurants_by_cost_ratings(restaurant_dataset, city, cost):
    print("filtering based on city and cost")
    restaurant_dataset = restaurant_dataset[restaurant_dataset['lokasi_kota']==city]
    restaurant_dataset = restaurant_dataset[restaurant_dataset['level_price']<=cost]
    return restaurant_dataset

"""Get closests for each centroid"""


def get_distance(lati1, long1, lati2, long2):
    return haversine((lati1, long1), (lati2, long2), unit=Unit.KILOMETERS)

def get_restaurants_each_day(place_recommendation_dataset, centroid, restaurant_dataset_original, city, cost):
    print("GETTING RESTAURANTS RECOMMENDATION")
    restaurant_dataset = filter_restaurants_by_cost_ratings(restaurant_dataset_original, city, cost)

    print("finding nearests restaurants for each day")
    resturants_for_each_day = []
    cluster_label_order = [] 
    # get the list of centroid order
    print('centroids ordering')
    for day in place_recommendation_dataset:
      # get the first one on every day
      cluster_label_order.append(day.iloc[0]['cluster_label'])
      print(day.iloc[0]['cluster_label'])
      
    for label in cluster_label_order:
      # for index, place in day.iterrows(): 
      lat_tourism = centroid[label][0]
      long_tourism = centroid[label][1]

      # finding the distance for every centroid and restaurant_place
      # assign it to a new row in the restaurant dataset
      restaurant_dataset[f"distance_cluster_{label}"] = restaurant_dataset.apply(
          lambda x: get_distance(x['geometry_location_lat'],x['geometry_location_lng'],
                                  lat_tourism, long_tourism) ,axis=1)
    # create a feature which averages the distance to all centroids
    restaurant_dataset["distance_avg"] = restaurant_dataset.apply(
          lambda x: np.mean( [ x[f"distance_cluster_{i}"]  for i in cluster_label_order ]) ,axis=1)
    
    for label in cluster_label_order:
      # sort by the closest to the centroid
      restaurant_dataset.sort_values(f"distance_cluster_{label}",axis=0,inplace=True)
      # make a new columns to save the distance to this cluster/day
      restaurant_dataset["distance_part_of_cluster"] = restaurant_dataset.apply(lambda x: x[f"distance_cluster_{label}"], axis=1) 

      # take 4 closest for each day  
      resturants_for_each_day.append(restaurant_dataset.iloc[:4]) 
      # to ensure no duplicate choice
      restaurant_dataset.drop(restaurant_dataset.index[0:4], inplace=True)
    
    print("...generated restaurants recommendations for each day")
    return resturants_for_each_day


"""### Find the closest surrounding hotels/accomodations to all of the places, not just each day

Filter by cost and ratings
"""

def filter_acommodations_by_cost_ratings(acommodations_dataset, city, cost):
  print("filtering based on city and cost")
  acommodations_dataset = acommodations_dataset[acommodations_dataset['lokasi_kota']==city]
  acommodations_dataset = acommodations_dataset[acommodations_dataset['level_price']<=cost]
  return acommodations_dataset

def get_acommodations(place_recommendation_dataset, centroid, acommodation_dataset_original, city, cost):
    print("GETTING ACOMMODATIONS RECOMMENDATION")
    acommodation_dataset = filter_restaurants_by_cost_ratings(acommodation_dataset_original, city, cost)

    print("finding nearests acommodations")
    cluster_label_order = [] 
    # get the list of centroid order
    print('centroids ordering')
    for day in place_recommendation_dataset:
      # get the first one on every day
      cluster_label_order.append(day.iloc[0]['cluster_label'])
      print(day.iloc[0]['cluster_label'])
      
    for label in cluster_label_order:
      # for index, place in day.iterrows(): 
      lat_tourism = centroid[label][0]
      long_tourism = centroid[label][1]

      # finding the distance for every centroid and restaurant_place
      # assign it to a new row in the restaurant dataset
      acommodation_dataset[f"distance_cluster_{label}"] = acommodation_dataset.apply(
          lambda x: get_distance(x['geometry_location_lat'],x['geometry_location_lng'],
                                  lat_tourism, long_tourism) ,axis=1)
    # create a feature which averages the distance to all centroids
    acommodation_dataset["distance_avg"] = acommodation_dataset.apply(
          lambda x: np.mean( [ x[f"distance_cluster_{i}"]  for i in cluster_label_order ]) ,axis=1)
    
  
    # sort by the closest average distance to all centroid
    acommodation_dataset.sort_values(f"distance_avg",axis=0,inplace=True)

    # take the first 7
    print("...generated acommodations recommendations")
    return acommodation_dataset[:10]

# acommodations_recommendations =  get_acommodations(tourism_lists_each_day, centroid, dataset_acommodation, CITY, COST)

"""### Calculate cost minimum and maximum"""

def calculate_cost_min_max(tourism_recommendations_each_day, restaurants_recommendations_each_day):
  # calculate cost for tourism
  print("CALCULATING COSTS")
  min_tourism = 0
  max_tourism = 0
  # print(cost_sheets)
  for df in tourism_recommendations_each_day:
    for index,place in df.iterrows():
        min_tourism += place['cost_range_min']
        max_tourism += place['cost_range_max']

  print("min_tourism",min_tourism)
  print("max_tourism",max_tourism)
  min_restaurant = 0
  max_restaurant = 0

  # calculate cost for restaurants
  for df in restaurants_recommendations_each_day:
    for index,place in df.iterrows():
        min_restaurant += place['cost_range_min']
        max_restaurant += place['cost_range_max']
  print("min_restaurant",min_restaurant)
  print("max_restaurant",max_restaurant)
  total_min_cost =  min_tourism + min_restaurant
  total_max_cost =  max_tourism + max_restaurant

  print()
  print("total_min_cost",total_min_cost)
  print("total_max_cost",total_max_cost)
  return total_min_cost, total_max_cost

# calculate_cost_min_max(tourism_lists_each_day, restaurants_recommendations_each_day)

"""### FINAL PREDICTION COMBINATION FLOW"""

# Take input query, location, n_days, cost_level
def predict_give_places_recommendations(dataset_tourism, dataset_restaurant, dataset_acommodation, query, city, n_days, n_people, cost):
  tfidf = TfidfVectorizer()

  # filtering #1, city and cost
  dataset_tourism = filter_result_by_location_cost(dataset_tourism, city, cost)

  # clean query and give recommends with content-based filtering
  cos_list, id = recommendation_with_query(dataset_tourism, tfidf, query, city)

  # take the first n recommendations
  places_list, sorted_scores_list =  get_the_first_n_recommendation_places_and_scores(dataset_tourism, cos_list, id, n_days*8)

  # clustering
  dataset_clustered, centroid, labels = clustering_places_kmeans(places_list, n_days)

  # filtering #2, n of day
  tourism_lists_each_day, num_of_places_per_day = filter_num_of_places_in_a_day_sorted(dataset_clustered)
  
  # give restaurants recommendations for each day, filtered by cost and distance 
  restaurants_recommendations_each_day = get_restaurants_each_day(tourism_lists_each_day, centroid, dataset_restaurant, city, cost)
  # give acommodations recommendations, filtered by cost
  acommodations_recommendations =  get_acommodations(tourism_lists_each_day, centroid, dataset_acommodation, city, cost)

  # calculate cost
  cost_minimum, cost_maximum = calculate_cost_min_max(tourism_lists_each_day, restaurants_recommendations_each_day)

  # select the output columns
  tourism_lists_each_day_fin = []
  restaurants_recommendations_each_day_fin = []
  # print(tourism_lists_each_day[0].columns)
  for df in tourism_lists_each_day:
    tourism_lists_each_day_fin.append(df[['place_id', 'Place_Name','image_link', 'Description', 'Category', 'City', 'Rating','Lat', 'Long', 'cost_range_min', 'cost_range_max']])
  
  # print(restaurants_recommendations_each_day[0].columns)
  for df in restaurants_recommendations_each_day:
    restaurants_recommendations_each_day_fin.append(df[['place_id', 'link_restaurant',
       'name', 'formatted_address', 'geometry_location_lat','geometry_location_lng', 'tipe_makanan', 'level_price', 'rating_ten', 'cost_range_min', 'cost_range_max','distance_part_of_cluster']])
  # print(acommodations_recommendations.columns)
  acommodations_recommendations = acommodations_recommendations[['place_icon_image',
       'acommodation_name_concatenated', 'formatted_address', 'geometry_location_lat', 'geometry_location_lng', 'acommodation_type', 'lokasi', 'rate_level', 'rating.1',
       'num_of_reviews', 'price_per_night', 'distance_avg']]
  print("...RECOMMENDATION PROCESS DONE")
  # Give output places & restaurants to visit that's already sorted into different days, recommended acommodations, cost_minimum, cost_maximum  
  return tourism_lists_each_day_fin, restaurants_recommendations_each_day_fin, acommodations_recommendations, cost_minimum, cost_maximum, cost_minimum*n_people,cost_maximum*n_people


if __name__ == "__main__":
  """### EXAMPLE PREDICTION"""

  N_DAYS = 2
  CITY = "Bandung"
  QUERY = "tempat belajar dan mengerjakan tugas sambil Menikmati musik dan alam"
  COST = 1
  N_PEOPLE = 3


  tourism_lists_each_day_fin,restaurants_recommendations_each_day_fin, acommodations_recommendations, cost_minimum_per_person, cost_maximum_per_person, cost_minimum, cost_maximum = predict_give_places_recommendations(dataset_tourism,dataset_restaurant,dataset_acommodation,QUERY, CITY, N_DAYS, N_PEOPLE, COST)

  print(tourism_lists_each_day_fin[0])

""" INI TOLONG DI RETURN AJA SEMUANYA, HARUSNYA SEMUA BISA DIGUNAKAN """